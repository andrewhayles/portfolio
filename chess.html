<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Page-specific title from the markdown file -->
    <title>Chess Database Analysis - Andrew Hayles</title>
    
    <!-- Load Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Load custom fonts -->
    <link href="https://fonts.googleapis.com/css2?family=DM+Mono:wght@400;500&family=Inter:wght@400;700&display=swap" rel="stylesheet">
    
    <script>
      // --- Custom Tailwind Config (consistent with index.html) ---
      tailwind.config = {
        darkMode: 'media', 
        theme: {
          extend: {
            fontFamily: {
              sans: ['Inter', 'sans-serif'],
              mono: ['"DM Mono"', 'monospace'],
            },
            colors: {
              light: '#ffffff',
              onLight: '#050806',
              dark: '#000000',
              onDark: '#FBFFF2',
              primary: '#0804F6',
              onPrimary: '#FBFFF2',
              secondary: '#FE491F',
              onSecondary: '#050806',
              complementary: '#565862',
              onComplementary: '#FBFFF2',
            }
          }
        }
      }
    </script>
    
    <style>
      /* --- Prose styling (consistent with index.html) --- */
      .prose { @apply text-onLight dark:text-onDark; }
      .prose h1, .prose h2, .prose h3, .prose h4 { @apply font-mono uppercase font-normal mb-4 mt-8 text-onLight dark:text-onDark; }
      .prose h1 { @apply text-3xl md:text-4xl; }
      .prose h2 { @apply text-2xl md:text-3xl; }
      .prose h3 { @apply text-xl md:text-2xl; }
      .prose h4 { @apply text-lg md:text-xl; }
      .prose p { @apply mb-5 leading-relaxed text-lg; }
      .prose a { @apply text-primary dark:text-secondary hover:underline; }
      .prose img { @apply rounded-lg shadow-md my-6 max-w-full h-auto mx-auto; }
      /* Added language-text for the ACPL report box */
      .prose pre { @apply bg-gray-100 dark:bg-gray-900 text-onLight dark:text-onDark p-4 rounded-lg overflow-x-auto; }
      .prose pre.language-text { @apply bg-gray-50 dark:bg-gray-800 border border-gray-200 dark:border-gray-700; }
      .prose code { @apply font-mono text-sm; }
      .prose blockquote { @apply border-l-4 border-primary dark:border-secondary pl-4 italic my-4; }
      .prose ul, .prose ol { @apply list-disc list-inside mb-4 pl-4; }
      .prose hr { @apply border-gray-200 dark:border-gray-700 my-8; }
    </style>
</head>

<body class="bg-light dark:bg-dark font-sans text-onLight dark:text-onDark transition-colors duration-300">
  <div class="flex flex-col min-h-screen">

    <!-- === Header (Consistent with index.html) === -->
    <header class="bg-light/95 dark:bg-dark/90 backdrop-blur-sm sticky top-0 z-50 border-b border-gray-200 dark:border-gray-800">
      <nav class="container mx-auto px-4 py-4 flex justify-between items-center">
        <!-- Site Title -->
        <a href="index.html" class="text-xl font-mono uppercase font-bold text-onLight dark:text-onDark hover:text-primary dark:hover:text-secondary">
          Home
        </a>
        
        <!-- Nav Links -->
        <div class="flex items-center space-x-4 md:space-x-6">
          <a href="about.html" class="text-onLight dark:text-onDark hover:text-primary dark:hover:text-secondary font-mono uppercase text-sm font-medium">About</a>
          <a href="https://github.com/andrewhayles/" target="_blank" rel="noopener noreferrer" class="text-onLight dark:text-onDark hover:text-primary dark:hover:text-secondary" aria-label="GitHub">
            <svg class="w-6 h-6" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M12 2C6.477 2 2 6.477 2 12c0 4.418 2.865 8.165 6.839 9.49.5.092.682-.217.682-.483 0-.237-.009-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.11-4.555-4.951 0-1.093.39-1.988 1.03-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0 1 12 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.026 2.747-1.026.546 1.379.202 2.398.1 2.65.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.942.359.309.678.92.678 1.856 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.001 10.001 0 0 0 22 12c0-5.523-4.477-10-10-10Z" clip-rule="evenodd" /></svg>
          </a>
        </div>
      </nav>
    </header>

    <!-- === Main Content === -->
    <main class="flex-grow">
      <!-- Using max-w-4xl for a comfortable reading width for articles -->
      <div class="container mx-auto px-4 py-16 md:py-24 max-w-4xl prose">
        
        <!-- Title from frontmatter -->
        <h1>Chess Database Analysis</h1>


        <!-- Main content from markdown body -->
        <h3>Brief Analysis of Personal Chess Database</h3>
        <p>Andrew Hayles</p>
        <p>A similar methodology to that which is used in this report is employed in the book titled “Chess Two New Rating Systems: The 2022 Best Players in History (English Edition)” available on Amazon.com by Hindemburg Melão Jr. He also found a strong relationship between “hit count” in the sense found in this report and chess engine ELO strength.</p>
        
        <p>This is based on an analysis of 84 chess games of mine on chess.com from recent months. I analyzed all the games from move 5 onward (this produces 2,569 positions) with a very strong stockfish engine on its maximum strength setting with a large analysis depth and generous time limit to find the "perfect" move in each position of each game. Then I analyzed the hit count of each of these lesser strength engines and then applied optimized weights to the hits (1st best move gets a 1, 2nd best move gets a 1, third best move gets a 0.55). In this particular analysis I suppressed the performance of the stockfish 1950 - 2050 rated engines (three engines) by reducing the depth of their analysis of each move and I removed the highest rated stockfish engine (3644 ELO) because it wasn't performing as well as it should. This rating estimate for me based on my personal hit count (adjusted with weights) is approximately what my rating is right now on chess.com. So this suggests that my performance is reflected accurately in my rating. I am not performing at a much higher level than my rating suggests, or lower level. Extremely interesting is when I run the analysis on only moves 10-14. My rating is very low. I don't tend to make the right moves at this stage of the game. I need to work on that.</p>
        
        <!-- Replaced local image with placeholder -->
        <img src="/images/image1.webp" alt="Graph of hit count score versus rating for analysis over all moves from move 5" style="max-width: 100%; height: auto; margin: 25px;">
        
        <p>Here is the optimized graph for moves 10-14 only (inclusive). As can be seen, my rating is lower than the lowest rated engine. Suggesting I am very poor at selecting the "best" move at this stage of the game.</p>
        
        <!-- Replaced local image with placeholder -->
        <img src="/images/image2.webp" alt="Graph of hit count score versus rating for analysis for moves 10-14 only" style="max-width: 100%; height: auto; margin: 25px;"">
        
        <p>Using SQL I applied a filter to my games to see if it made any difference in how long the game lasted on my win percentage. It turns out it often seems to, but mostly doesn't matter. However, since I found some interesting results in SQL I decided to do a more thorough analysis using Python and a statistical significance test called Chi-Square Test for Independence. This is a good test for determining if there is indeed a relationship between categorical variables or not. So I used Google Gemini to help build a script that would test different ranges of game-lengths for a statistically significant relationship. This is what it found:</p>
        
        <!-- Replaced local image with placeholder -->
        <img src="/images/pvalues_chisquare.webp" alt="Graph of p-values" style="max-width: 100%; height: auto; margin: 25px;">
        
        <p>Essentially, there is no meaningful relationship between the length of the game and my percentage of wins except when one considers games that are between 0-49 plies (about 25 moves for both players) and games greater than or equal to 50 plies in length. When you consider the specific win percentages for this range (see below):</p>
        
        <!-- Replaced local image with placeholder -->
        <img src="/images/SQLoutput_chess_percentages.png" alt="Table of win percentages" style="max-width: 100%; height: auto; margin: 25px;">
        
        <p>...one can easily see that when the games are shorter, I lose much more often, and when the games are longer, I normally win. This could mean many things, like for example I am very skilled in the late middle-game to end game stages. Or perhaps I am very poor in the opening. I think what this truly reflects is that when I lose games it is normally as a result of making some foolish error in the early stages of the game which is already decisive by the 25th move, so that I normally resign or am checkmated in these games. The vast majority of the games (over 2/3) that I don't make foolish mistakes in the opening and early middle-game in, I win.</p>
        
        <p>Upon analyzing my most recent chess games, I got a slightly higher estimate of my rating (consistent with my increased rating on chess.com and my efforts to improve my performance at chess). The only problem I found was that I was getting a very narrow range of performance out of the engines I was using to make the estimate, which is not ideal. It would be nice to have a wide range of performances from a wide range of engine strengths instead of a rating range of 1000 points corresponding to a hit proportion score of 0.6 to 0.7 only. I made many efforts to try to achieve this, using node limits on the engines, using specific rating limits, using time limits, using depth limits, and combinations of the previously mentioned limits. All of these efforts failed and so I decided to switch my measurement method.</p>
        
        <p>I decided to use a different metric called "average centipawn loss" to estimate my performance rating. This is a widely accepted metric in the chess community and not nearly as controversial of a method to use for estimating one's performance. I utilized Google Gemini to help build a script to analyze my games using the Stockfish engine for centipawn loss (CPL) and calculate an output file with my results. This is what resulted:</p>
        
        <!-- Formatted ACPL report -->
        <h4>ACPL (Average Centipawn Loss) Report for: Desjardins373</h4>
		<hr>
		<pre><code class="language-text">Calculated ACPL: 54.09
		Analyzed Moves: 3683

		Estimated Skill Level: Expert / Club Player Level</code></pre>
		<hr>
		<h4>What is ACPL?</h4>
		<hr>
		<p>Average Centipawn Loss (ACPL) is a metric used to measure the accuracy of chess moves. A "centipawn" is 1/100th of a pawn. ACPL represents the average number of centipawns you lost per move compared to the engine's best move. A lower ACPL means your moves were closer to the computer's top choices, indicating higher accuracy.</p>
		<hr>
		<h4>Practical ACPL Tiers (Perspective):</h4>
		<hr>
		<p>These ranges are approximations, particularly for games with faster time controls, but they provide a useful reference point for self-assessment.</p>
		<ul>
		  <li><strong>10-20: Super Grandmaster (2700+ Elo):</strong> World-class precision, frequently dropping into single digits in top-level matches.</li>
		  <li><strong>20-25: Grandmaster (2500-2700 Elo):</strong> Exceptional consistency and a low frequency of significant errors.</li>
		  <li><strong>25-35: Master (2200-2400 Elo):</strong> Strong, professional-level play, indicating a solid performance.</li>
		  <li><strong>30-60: Expert / Club Player (1800-2200 Elo):</strong> Represents strong amateur and tournament players. Averages in the 50s and 60s are common, especially in faster time controls.</li>
		  <li><strong>60-120: Intermediate Player (1200-1800 Elo):</strong> A typical range for the large cohort of intermediate players. Games are often decided by more obvious tactical mistakes.</li>
		  <li><strong>100+: Novice / Beginner (&lt;1200 Elo):</strong> Fundamental blunders are common. Scores of 150 or higher are commonplace. An ACPL over 300 suggests a player is very new to the game.</li>
		</ul>
		<p><em>Note: These ranges are approximate and can be affected by the complexity of the games, time control, and the depth of the analysis engine.</em></p>
		<hr>

        <p>So, in contrast to many claims by friends and family that I am "...like a chess master" the true measurements don't bear this out, I fall significantly short of playing at this level of skill.</p>
        
        <!-- 
          This section inserts the code from the 'code:' block in the frontmatter.
          It's manually converted into HTML <p> and <pre><code> blocks for correct styling.
        -->
        <h2>Project Code &amp; Scripts</h2>

<pre><code class="language-python">import chess
import chess.pgn
import chess.engine
import pandas as pd
import time
import sys
import os
import json
from scipy.stats import linregress
from tqdm import tqdm
import subprocess
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

# ==============================================================================
# --- Configuration ---
# ==============================================================================

# --- Player and Game Configuration ---
PLAYER_NAME_IN_PGN = "Desjardins373"
PLAYER_PGN_PATH = "goalgames.pgn" 

# --- Optimal Method Configuration ---
CHOSEN_METHOD = {
  "name": "Top 3 Moves, Linear Weights",
  "num_moves": 3,
  "weights": [1.0, 0.5, 0.25]
}

# --- Analysis Control ---
START_MOVE = 10
POSITIONS_PER_GAME = 5

# --- File Paths ---
ENGINES_CSV_PATH = "real_engines.csv"
GRANULAR_LOG_PATH = "granular_analysis_log.csv"
OUTPUT_GRAPH_PATH = "player_rating_estimate_final.png"
ORACLE_CACHE_PATH = "oracle_cache.json"
STATUS_HISTORY_LOG_PATH = "status_history.csv"

# --- Default Engine Settings ---
ORACLE_ENGINE_NAME = "stockfish_full_1"
ORACLE_ANALYSIS_DEPTH = 22
ORACLE_ANALYSIS_TIMEOUT = 600
ENGINE_THREADS = 2
DEFAULT_TEST_DEPTH = 9
DEFAULT_TEST_TIMEOUT = 0.05

# --- *** NEW: Engine-Specific Configuration Overrides *** ---
# Use this to "handicap" engines that are too strong for the test.
# The script will use the DEFAULT settings unless an engine's name is found here.
# You can override 'depth', 'time', or both.
ENGINE_CONFIG_OVERRIDES = {
  "stockfish_elo_1950": {"depth": 4, "time": 0.05},
  "stockfish_elo_2007": {"depth": 4, "time": 0.05},
  "stockfish_elo_2050": {"depth": 4, "time": 0.05},
  # Example for another engine:
  # "dragon": {"depth": 10, "time": 0.02} 
}

# ==============================================================================
# --- Core Logic ---
# ==============================================================================

def open_engine(path):
  """Opens a chess engine, handling potential startup issues."""
  startupinfo = None
  if os.name == 'nt':
    startupinfo = subprocess.STARTUPINFO()
    startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
  stderr_pipe = subprocess.DEVNULL if "leela" in path.lower() else None
  try:
    return chess.engine.SimpleEngine.popen_uci(path, stderr=stderr_pipe, startupinfo=startupinfo)
  except Exception as e:
    print(f"Error opening engine at {path}: {e}", file=sys.stderr)
    return None

def get_positions_from_pgn(pgn_path, player_name, positions_per_game, start_move):
  """Extracts a list of FENs and the player's move from a PGN file."""
  if not os.path.exists(pgn_path):
    print(f"Error: PGN file not found at '{pgn_path}'.", file=sys.stderr)
    return []
    
  positions = []
  print(f"Processing PGN: {os.path.basename(pgn_path)} for player: {player_name}")
  with open(pgn_path, 'r', errors='ignore') as pgn:
    while True:
      try:
        game = chess.pgn.read_game(pgn)
        if game is None: break
        
        white_player = game.headers.get("White", "")
        black_player = game.headers.get("Black", "")
        if player_name not in white_player and player_name not in black_player:
          continue

        board = game.board()
        positions_in_this_game = 0
        is_player_white = player_name in white_player

        for move in game.mainline_moves():
          is_player_turn = (is_player_white and board.turn == chess.WHITE) or \
                   (not is_player_white and board.turn == chess.BLACK)
          
          if is_player_turn and board.fullmove_number >= start_move:
            if positions_per_game is not None and positions_in_this_game >= positions_per_game:
              break
            positions.append({"fen": board.fen(), "actual_move": move.uci()})
            positions_in_this_game += 1
          board.push(move)
      except Exception as e:
        print(f"Skipping a game due to a parsing error: {e}", file=sys.stderr)
        continue
  return positions

def get_move_score(move_to_check, oracle_moves, weights):
  """Calculates the score for a move based on the oracle's ranking."""
  score = 0.0
  for i, oracle_move in enumerate(oracle_moves):
    if move_to_check == oracle_move:
      score = weights[i]
      break
  return score

def log_status_history(log_path, items_completed, total_items, overall_r2, overall_rating, inst_r2, inst_rating):
  """Appends a new status line to a historical CSV log file."""
  try:
    header_needed = not os.path.exists(log_path)
    with open(log_path, 'a', newline='') as f:
      if header_needed:
        f.write("timestamp,items_completed,total_items,overall_r_squared,overall_rating,instantaneous_r_squared,instantaneous_rating\n")

      timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
      overall_rating_str = f"{overall_rating:.2f}" if overall_rating is not None else "N/A"
      overall_r2_str = f"{overall_r2:.4f}" if overall_r2 is not None else "N/A"
      inst_rating_str = f"{inst_rating:.2f}" if inst_rating is not None else "N/A"
      inst_r2_str = f"{inst_r2:.4f}" if inst_r2 is not None else "N/A"
      
      f.write(f"{timestamp},{items_completed},{total_items},{overall_r2_str},{overall_rating_str},{inst_r2_str},{inst_rating_str}\n")
  except Exception as e:
    print(f"Warning: Could not write to status history log: {e}", file=sys.stderr)


def calculate_rating_from_scores(scores_df, player_score, benchmark_engine_info):
  """Calculates R-squared and rating from a dataframe of scores."""
  engine_scores_df = scores_df[scores_df['engine_name'] != 'player']
  if len(engine_scores_df) < 2 or engine_scores_df['score'].nunique() < 2:
    return None, None
    
  merged_df = pd.merge(benchmark_engine_info, engine_scores_df, on='engine_name')
  if len(merged_df) < 2:
    return None, None

  slope, intercept, r_value, _, _ = linregress(merged_df['score'], merged_df['rating'])
  r_squared = r_value ** 2
  estimated_rating = (slope * player_score) + intercept
  return r_squared, estimated_rating


def main():
  """Main function to run the position-by-position analysis."""
  print("--- Starting Position-by-Position Rating Estimation Script ---")
  
  # --- 1. Load Engine and Game Data ---
  try:
    engines_df = pd.read_csv(ENGINES_CSV_PATH)
  except FileNotFoundError:
    print(f"Error: Engines CSV file not found at '{ENGINES_CSV_PATH}'. Exiting.", file=sys.stderr)
    return

  all_player_positions = get_positions_from_pgn(PLAYER_PGN_PATH, PLAYER_NAME_IN_PGN, POSITIONS_PER_GAME, START_MOVE)
  if not all_player_positions:
    print(f"No positions found for player '{PLAYER_NAME_IN_PGN}'. Exiting."); return
  print(f"Found {len(all_player_positions)} total positions for player '{PLAYER_NAME_IN_PGN}'.")

  # --- 2. Load Caches and Check Progress ---
  oracle_row = engines_df[engines_df['engine_name'] == ORACLE_ENGINE_NAME]
  if oracle_row.empty:
    print(f"Error: Oracle engine '{ORACLE_ENGINE_NAME}' not found in {ENGINES_CSV_PATH}."); return
  
  benchmark_engine_info = engines_df[engines_df['engine_name'] != ORACLE_ENGINE_NAME]
  engine_names = list(benchmark_engine_info['engine_name'])
  
  oracle_cache = {}
  if os.path.exists(ORACLE_CACHE_PATH):
    with open(ORACLE_CACHE_PATH, 'r') as f:
      try:
        oracle_cache = json.load(f)
        print(f"Loaded {len(oracle_cache)} positions from Oracle cache.")
      except json.JSONDecodeError: print("Oracle cache file is corrupted, starting fresh.")

  # --- 3. Initialize In-Memory Log and Identify Work to Do ---
  full_log_df = pd.DataFrame(columns=['fen', 'engine_name', 'score'])
  if os.path.exists(GRANULAR_LOG_PATH):
    try:
      full_log_df = pd.read_csv(GRANULAR_LOG_PATH)
      if not full_log_df.empty:
        print(f"Found {len(full_log_df)} previously completed analyses in '{GRANULAR_LOG_PATH}'.")
    except (pd.errors.EmptyDataError, KeyError):
       print(f"'{GRANULAR_LOG_PATH}' is empty or malformed. Starting fresh.")
  
  completed_items = set(zip(full_log_df['fen'], full_log_df['engine_name'])) if not full_log_df.empty else set()

  # --- 4. Phase 1: Update Oracle Cache ---
  all_fens_to_process = {p['fen'] for p in all_player_positions}
  fens_needing_oracle = list(all_fens_to_process - set(oracle_cache.keys()))
  
  if fens_needing_oracle:
    print(f"\n--- Phase 1: Caching {len(fens_needing_oracle)} new positions with Oracle Engine ---")
    oracle_engine = open_engine(oracle_row.iloc[0]['path'])
    if not oracle_engine:
      print("Could not start Oracle engine. Exiting.", file=sys.stderr); return
    oracle_engine.configure({"Threads": ENGINE_THREADS})
    
    for fen in tqdm(fens_needing_oracle, desc="Oracle Caching"):
      board = chess.Board(fen)
      limit = chess.engine.Limit(depth=ORACLE_ANALYSIS_DEPTH, time=ORACLE_ANALYSIS_TIMEOUT)
      analysis = oracle_engine.analyse(board, limit, multipv=CHOSEN_METHOD['num_moves'])
      oracle_moves = [info['pv'][0].uci() for info in analysis]
      oracle_cache[fen] = oracle_moves
      with open(ORACLE_CACHE_PATH, 'w') as f: json.dump(oracle_cache, f)
    oracle_engine.quit()
    print("Oracle caching complete.")
  else:
    print("\nOracle cache is already up-to-date.")

  # --- 5. Main Analysis Loop ---
  items_to_analyze = []
  for pos in all_player_positions:
    if (pos['fen'], 'player') not in completed_items:
      items_to_analyze.append({'type': 'player', 'fen': pos['fen'], 'actual_move': pos['actual_move']})
    for name in engine_names:
      if (pos['fen'], name) not in completed_items:
        items_to_analyze.append({'type': 'engine', 'fen': pos['fen'], 'engine_name': name})
  
  num_required_per_pos = len(engine_names) + 1
  total_analyses_in_pgn = len(all_player_positions) * num_required_per_pos

  if not items_to_analyze:
    print("All analyses are already complete. Generating final report.")
  else:
    print(f"\n--- Phase 2: Performing {len(items_to_analyze)} new analyses ---")
    benchmark_engines = {}
    for _, row in benchmark_engine_info.iterrows():
      engine = open_engine(row['path'])
      if engine:
        engine.configure({"Threads": ENGINE_THREADS})
        benchmark_engines[row['engine_name']] = engine
    
    if not benchmark_engines and any(item['type'] == 'engine' for item in items_to_analyze):
      print("Failed to initialize any benchmark engines. Exiting.", file=sys.stderr); return

    pbar = tqdm(total=len(items_to_analyze), desc="Analyzing Items", unit="item")
    granular_header_written = len(completed_items) > 0

    for item in items_to_analyze:
      fen, score = item['fen'], 0
      board = chess.Board(fen)
      oracle_moves = oracle_cache[fen]
      engine_name_for_log = ""

      if item['type'] == 'player':
        score = get_move_score(item['actual_move'], oracle_moves, CHOSEN_METHOD['weights'])
        engine_name_for_log = 'player'
      elif item['type'] == 'engine':
        engine_name_for_log = item['engine_name']
        try:
          # --- Get specific config for this engine ---
          config = ENGINE_CONFIG_OVERRIDES.get(engine_name_for_log, {})
          depth = config.get('depth', DEFAULT_TEST_DEPTH)
          timeout = config.get('time', DEFAULT_TEST_TIMEOUT)
          limit = chess.engine.Limit(depth=depth, time=timeout)
          
          result = benchmark_engines[engine_name_for_log].play(board, limit)
          score = get_move_score(result.move.uci(), oracle_moves, CHOSEN_METHOD['weights'])
        except (chess.engine.EngineError, chess.engine.EngineTerminatedError) as e:
          print(f"\nWarning: Engine '{engine_name_for_log}' failed on FEN {fen}. Error: {e}", file=sys.stderr)
          score = 0.0
      
      new_row_dict = {'fen': fen, 'engine_name': engine_name_for_log, 'score': score}
      
      # --- Append to CSV log file ---
      log_entry_df = pd.DataFrame([new_row_dict])
      log_entry_df.to_csv(GRANULAR_LOG_PATH, mode='a', header=not granular_header_written, index=False)
      granular_header_written = True
      
      # --- FIX: Append new row to in-memory DataFrame efficiently ---
      full_log_df.loc[len(full_log_df)] = new_row_dict
      
      # --- Recalculate and Log Status on Every Item ---
      inst_r2, inst_rating = None, None
      items_for_this_fen = full_log_df[full_log_df['fen'] == fen]
      if len(items_for_this_fen) == num_required_per_pos:
        inst_player_score_row = items_for_this_fen[items_for_this_fen['engine_name'] == 'player']
        if not inst_player_score_row.empty:
          inst_player_score = inst_player_score_row.iloc[0]['score']
          inst_r2, inst_rating = calculate_rating_from_scores(items_for_this_fen, inst_player_score, benchmark_engine_info)

      avg_scores_df = full_log_df.groupby('engine_name')['score'].mean().reset_index()
      overall_player_score_row = avg_scores_df[avg_scores_df['engine_name'] == 'player']
      overall_r2, overall_rating = None, None
      if not overall_player_score_row.empty:
        overall_player_score = overall_player_score_row.iloc[0]['score']
        overall_engine_scores_df = avg_scores_df[avg_scores_df['engine_name'] != 'player']
        overall_r2, overall_rating = calculate_rating_from_scores(overall_engine_scores_df, overall_player_score, benchmark_engine_info)

      log_status_history(STATUS_HISTORY_LOG_PATH, len(full_log_df), total_analyses_in_pgn, overall_r2, overall_rating, inst_r2, inst_rating)
      pbar.update(1)

    pbar.close()
    print("\nClosing benchmark engines...")
    for engine in benchmark_engines.values(): engine.quit()

  # --- 6. Final Report and Graph ---
  # (This section remains unchanged)
  print("\n--- Generating Final Report from Granular Log ---")
  if not os.path.exists(GRANULAR_LOG_PATH):
    print("Log file not found. Cannot generate report."); return
    
  final_log_df = pd.read_csv(GRANULAR_LOG_PATH)
  if final_log_df.empty:
    print("Log file is empty. Cannot generate report."); return

  avg_scores_df = final_log_df.groupby('engine_name')['score'].mean().reset_index()
  avg_scores_df.rename(columns={'score': 'average_hit_score'}, inplace=True)

  player_avg_score_row = avg_scores_df[avg_scores_df['engine_name'] == 'player']
  if player_avg_score_row.empty:
    print("No data for player found in log. Cannot estimate rating."); return
  player_avg_score = player_avg_score_row.iloc[0]['average_hit_score']
  
  engine_avg_scores = avg_scores_df[avg_scores_df['engine_name'] != 'player']
  final_df = pd.merge(benchmark_engine_info, engine_avg_scores, on='engine_name')
  
  if len(final_df) < 2:
    print("Not enough benchmark engine data to create a rating estimate."); return

  if final_df['average_hit_score'].nunique() > 1:
    slope, intercept, r_value, _, _ = linregress(final_df['average_hit_score'], final_df['rating'])
    final_r_squared = r_value ** 2
    final_player_rating = (slope * player_avg_score) + intercept
    
    print(f"\n--- Final Results ---")
    print(f"Analysis complete over {final_log_df['fen'].nunique()} positions.")
    print(f"Player's Final Average Score: {player_avg_score:.4f}")
    print(f"Final Estimated Rating for {PLAYER_NAME_IN_PGN}: {final_player_rating:.0f}")
    print(f"Final R-squared: {final_r_squared:.4f}")

    plt.figure(figsize=(12, 8))
    sns.regplot(x='rating', y='average_hit_score', data=final_df, ci=None, line_kws={'color':'red', 'linestyle':'--'}, label='Engine Trend')
    for _, row in final_df.iterrows():
      plt.scatter(row['rating'], row['average_hit_score'], s=80)
      plt.text(row['rating'] + 10, row['average_hit_score'], row['engine_name'], fontsize=9)

    plt.scatter(final_player_rating, player_avg_score, color='gold', s=200, edgecolor='black', zorder=5, label=f'You ({PLAYER_NAME_IN_PGN})')
    plt.text(final_player_rating + 10, player_avg_score, 'You', fontsize=11, weight='bold')

    plt.title(f"Final Performance Analysis vs. Engine Rating\nMethod: {CHOSEN_METHOD['name']}", fontsize=16)
    plt.xlabel("Engine Rating (Elo)", fontsize=12)
    plt.ylabel("Average Hit Score", fontsize=12)
    plt.legend()
    plt.grid(True)
    plt.text(0.05, 0.95, f'$R^2 = {final_r_squared:.4f}$\nFinal Est. Rating: {final_player_rating:.0f}', 
         transform=plt.gca().transAxes, fontsize=14, verticalalignment='top', 
         bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.7))
         
    plt.savefig(OUTPUT_GRAPH_PATH)
    print(f"\nGraph saved to: {OUTPUT_GRAPH_PATH}")
  else:
    print("\nCould not generate final report: All benchmark engines have the same average score.")

  print("--- Script Finished ---")


if __name__ == "__main__":
  main()
</code></pre>

<p>### This is the end of the script.</p>

<p>The real_engines.csv file looks like this:</p>
<pre><code class="language-csv">engine_name,path,rating,uci_options,executable_name
maia_1100,"C:/Users/desja/Documents/Python_programs/chess_study/engines/leela_chess/m1100.bat",1100,"{}",m1100.bat
maia_1200,"C:/Users/desja/Documents/Python_programs/chess_study/engines/leela_chess/m1200.bat",1200,"{}",m1200.bat
maia_1300,"C:/Users/desja/Documents/Python_programs/chess_study/engines/leela_chess/m1300.bat",1300,"{}",m1300.bat
maia_1400,"C:/Users/desja/Documents/Python_programs/chess_study/engines/leela_chess/m1400.bat",1400,"{}",m1400.bat
maia_1500,"C:/Users/desja/Documents/Python_programs/chess_study/engines/leela_chess/m1500.bat",1500,"{}",m1500.bat
maia_1600,"C:/Users/desja/Documents/Python_programs/chess_study/engines/leela_chess/m1600.bat",1600,"{}",m1600.bat
maia_1700,"C:/Users/desja/Documents/Python_programs/chess_study/engines/leela_chess/m1700.bat",1700,"{}",m1700.bat
maia_1800,"C:/Users/desja/Documents/Python_programs/chess_study/engines/leela_chess/m1800.bat",1800,"{}",m1800.bat
maia_1900,"C:/Users/desja/Documents/Python_programs/chess_study/engines/leela_chess/m1900.bat",1900,"{}",m1900.bat
dragon,C:/Users/desja/Documents/Python_programs/chess_study/engines/dragon - 3625/dragon_05e2a7/Windows/dragon-64bit.exe,3625,"{}",dragon-64bit.exe
stockfish_full_test,C:/Users/desja/Documents/Python_programs/chess_study/engines/leela_chess/stockfish.exe,3644,"{""UCI_LimitStrength"": false}",stockfish.exe
stockfish_full_1,C:/Users/desja/Documents/Python_programs/chess_study/engines/leela_chess/stockfish.exe,3644,"{""UCI_LimitStrength"": false}",stockfish.exe
</code></pre>

<p>The .BAT files look like this (example for Maia 1100 rating):</p>
<pre><code class="language-batch">@echo off
C:\Users\desja\Documents\Python_programs\chess_study\engines\leela_chess\lc0.exe --backend=onnx-cpu --weights="C:\Users\desja\Documents\Python_programs\chess_study\engines\leela_chess\maia weights\m1100.pb"
</code></pre>

<p>The SQL script looked like this:</p>
<pre><code class="language-sql">SELECT
  CASE
    WHEN num_plies >= 0 AND num_plies < 50 THEN '0-49 plies'
    WHEN num_plies >= 50 THEN '50+ plies'
  END AS ply_range,
  COUNT(*) AS number_of_games,
  --This calculates the number of games won
  SUM(CASE WHEN termination LIKE "Desjardins373 won%" THEN 1 ELSE 0 END) AS games_won,
  -- This calculates the percentage
  ROUND((SUM(CASE WHEN termination LIKE "Desjardins373 won%" THEN 1 ELSE 0 END) * 100.0 / COUNT(*)),2) AS win_percentage
FROM
  games
GROUP BY
  ply_range
ORDER BY
  num_plies;
</code></pre>

<p>The python script that I used to perform the statistical significance testing looked like this:</p>
<pre><code class="language-python">import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency
import matplotlib.pyplot as plt
import seaborn as sns

def run_significance_tests_from_csv(csv_file_path):
  """
  This script loads game data from a CSV file, iterates through different ply
  cutoffs, performs a Chi-Squared test for each, and visualizes the results.

  Args:
    csv_file_path (str): The path to the input CSV file.
  """
  # --- 1. Load Data from CSV ---
  try:
    print(f"Loading game data from '{csv_file_path}'...")
    df = pd.read_csv(csv_file_path)
    # Ensure 'result' and 'num_plies' columns exist
    if 'result' not in df.columns or 'num_plies' not in df.columns:
      print("Error: CSV must contain 'result' and 'num_plies' columns.")
      return
    print(f"Successfully loaded {len(df)} games.\n")
  except FileNotFoundError:
    print(f"Error: The file '{csv_file_path}' was not found.")
    print("Please run the PGN to CSV converter script first.")
    return
  except Exception as e:
    print(f"An error occurred while reading the CSV: {e}")
    return

  # --- 2. Define Cutoffs and Run Tests ---
  # Define the ply values you want to use as cutoffs
  cutoffs = range(30, 101, 10) # Test cutoffs 30, 40, 50, ..., 100
  results = []

  print("Running Chi-Squared tests for various ply cutoffs...")
  for cutoff in cutoffs:
    # Create two groups based on the cutoff
    group_below = df[df['num_plies'] < cutoff]
    group_above = df[df['num_plies'] >= cutoff]

    # Create the 2x2 contingency table
    #   Rows: Below Cutoff, Above Cutoff
    #   Columns: Wins, Losses
    # We filter out draws for this win/loss analysis
    contingency_table = np.array([
      [len(group_below[group_below['result'] == 'win']), len(group_below[group_below['result'] == 'loss'])],
      [len(group_above[group_above['result'] == 'win']), len(group_above[group_above['result'] == 'loss'])]
    ])

    # Check if any group is empty to avoid errors with the test
    if contingency_table.sum(axis=1).any() == 0:
      print(f"Skipping cutoff {cutoff}: not enough data in one of the groups.")
      continue

    # Perform the Chi-Squared test
    chi2, p_value, _, _ = chi2_contingency(contingency_table)

    # Store the results
    results.append({
      'cutoff': f'&lt;{cutoff} vs >={cutoff}',
      'p_value': p_value,
      'chi2_stat': chi2
    })

  # Convert results to a DataFrame for easy viewing
  results_df = pd.DataFrame(results)

  # --- 3. Display Final Results ---
  print("\n--- Test Results ---")
  print(results_df.to_string(index=False))

  # --- 4. Visualize the P-Values ---
  if not results_df.empty:
    plt.style.use('seaborn-v0_8-whitegrid')
    fig, ax = plt.subplots(figsize=(12, 7))

    # Create the bar plot
    colors = ['#2E86C1' if p < 0.05 else '#AED6F1' for p in results_df['p_value']]
    bars = ax.bar(results_df['cutoff'], results_df['p_value'], color=colors)

    # Add a line for the significance threshold (p=0.05)
    ax.axhline(y=0.05, color='red', linestyle='--', linewidth=2, label='Significance Threshold (p=0.05)')

    # Add labels and titles
    ax.set_title('P-Values from Chi-Squared Tests at Different Ply Cutoffs', fontsize=16, pad=20)
    ax.set_xlabel('Ply Range Cutoff', fontsize=12)
    ax.set_ylabel('P-Value', fontsize=12)
    ax.legend()

    # Add p-value labels on top of each bar
    for bar in bars:
      yval = bar.get_height()
      ax.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{yval:.3f}', ha='center', va='bottom')

    # Improve layout and show plot
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()
  else:
    print("\nNo results to visualize.")

# --- Main execution block ---
if __name__ == '__main__':
  # The script now expects the CSV file generated by the PGN parser
  games_csv_file = 'games_data.csv'
  run_significance_tests_from_csv(games_csv_file)
</code></pre>

<p>End of statistical calculation script</p>
<p>Beginning of ACPL analysis script</p>

<pre><code class="language-python">"""
Stockfish PGN Analyzer (Overnight Version with ACPL Report)

This script can operate in two modes:

1. Full Analysis Mode:
   Analyzes a PGN file using a local Stockfish chess engine to calculate
   the centipawn loss (CPL) for each move. It's designed for long-running,
   deep analysis by distributing a total specified duration across all moves.
   After analysis, it saves an annotated PGN and generates an ACPL report
   for a specified player.

2. Report-Only Mode:
   If a PGN file has already been analyzed (i.e., it contains '[%cpl ...]'
   comments), this mode skips the engine analysis and directly generates
   the ACPL report for the specified player.

Prerequisites:
- Python 3.6+
- The `python-chess` library (`pip install python-chess`)
- A local Stockfish engine (only for Full Analysis Mode)

Usage:
# Full Analysis Example (9-hour analysis for player 'Player123'):
python cpl2.py --stockfish "C:\\stockfish\\stockfish.exe" --pgn "my_games.pgn" --duration 9 --player "Player123"

# Report-Only Example (for an already analyzed PGN):
python cpl2.py --pgn "my_games_analyzed.pgn" --player "Player123" --report-only
"""
import chess
import chess.pgn
import chess.engine
import argparse
import os
import sys
import time
import re

def get_stockfish_path():
    """Tries to find the Stockfish executable automatically."""
    if sys.platform == "win32":
        for path in os.environ["PATH"].split(os.pathsep):
            exe_path = os.path.join(path, "stockfish.exe")
            if os.path.exists(exe_path):
                return exe_path
        program_files = os.environ.get("ProgramFiles", "C:\\Program Files")
        common_path = os.path.join(program_files, "Stockfish", "stockfish.exe")
        if os.path.exists(common_path):
            return common_path
    else: # Linux or MacOS
        for path in os.environ["PATH"].split(os.pathsep):
            exe_path = os.path.join(path, "stockfish")
            if os.path.exists(exe_path) and os.access(exe_path, os.X_OK):
                return exe_path
    return None


def analyze_game_and_annotate(game, engine, analysis_limit):
    """
    Analyzes a single game, calculates CPL for each move, and adds it as a comment.
    Returns the modified game object and a list of CPL values for each move.
    """
    board = game.board()
    new_game = chess.pgn.Game()
    new_game.headers.update(game.headers)
    node = new_game
    cpl_data = []

    try:
        info = engine.analyse(board, limit=analysis_limit)
        score_before = info["score"].white()
    except (chess.engine.EngineError, chess.engine.EngineTerminatedError) as e:
        print(f"\nEngine error analyzing initial position: {e}", file=sys.stderr)
        return None, []

    for move in game.mainline_moves():
        board.push(move)
        try:
            info = engine.analyse(board, limit=analysis_limit)
            score_after = info["score"].white()
        except (chess.engine.EngineError, chess.engine.EngineTerminatedError) as e:
            print(f"\nEngine error during game: {e}", file=sys.stderr)
            break

        cpl = 0
        if score_before.is_mate() or score_after.is_mate():
             if score_before.mate() is not None and score_after.mate() is not None:
                 if (score_before.mate() > 0 and score_after.mate() > 0) or \
                    (score_before.mate() < 0 and score_after.mate() < 0):
                     cpl = 0
                 else:
                     cpl = 350
             else:
                 cpl = 350
        else:
            eval_before = score_before.score()
            eval_after = score_after.score()
            
            if board.turn == chess.BLACK:  # White just moved
                cpl = eval_before - eval_after
            else:  # Black just moved
                cpl = eval_after - eval_before
        
        cpl = max(0, int(cpl))
        cpl_data.append(cpl)

        node = node.add_variation(move)
        node.comment += f" [%cpl {cpl}]"
        
        score_before = score_after
        
    return new_game, cpl_data


def generate_acpl_report(player_name, acpl, total_moves, report_path):
    """Generates a text file with the ACPL analysis using the new ranges."""
    
    skill_level = "Unknown"
    if acpl &lt;= 20:
        skill_level = "Super Grandmaster / Grandmaster Level"
    elif acpl &lt;= 25:
        skill_level = "Grandmaster Level"
    elif acpl &lt;= 35:
        skill_level = "Master Level"
    elif acpl &lt;= 60:
        skill_level = "Expert / Club Player Level"
    elif acpl &lt;= 120:
        skill_level = "Intermediate Player"
    else: # acpl > 120
        skill_level = "Novice / Beginner Level"

    content = f"""
ACPL (Average Centipawn Loss) Report for: {player_name}
=====================================================

Calculated ACPL: {acpl:.2f}
Analyzed Moves: {total_moves}

Estimated Skill Level: {skill_level}

---
What is ACPL?
---
Average Centipawn Loss (ACPL) is a metric used to measure the accuracy of chess moves.
A "centipawn" is 1/100th of a pawn. ACPL represents the average number of centipawns
you lost per move compared to the engine's best move. A lower ACPL means your moves
were closer to the computer's top choices, indicating higher accuracy.

---
Practical ACPL Tiers (Perspective):
---
These ranges are approximations, particularly for games with faster time controls,
but they provide a useful reference point for self-assessment.

- 10-20:  Super Grandmaster (2700+ Elo): World-class precision, frequently dropping
          into single digits in top-level matches.

- 20-25:  Grandmaster (2500-2700 Elo): Exceptional consistency and a low
          frequency of significant errors.

- 25-35:  Master (2200-2400 Elo): Strong, professional-level play, indicating
          a solid performance.

- 30-60:  Expert / Club Player (1800-2200 Elo): Represents strong amateur and
          tournament players. Averages in the 50s and 60s are common,
          especially in faster time controls.

- 60-120: Intermediate Player (1200-1800 Elo): A typical range for the large
          cohort of intermediate players. Games are often decided by more
          obvious tactical mistakes.

- 100+:   Novice / Beginner (&lt;1200 Elo): Fundamental blunders are common. Scores
          of 150 or higher are commonplace. An ACPL over 300 suggests a player
          is very new to the game.

Note: These ranges are approximate and can be affected by the complexity of the games,
time control, and the depth of the analysis engine.
"""
    try:
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"Successfully generated ACPL report at '{report_path}'.")
    except IOError as e:
        print(f"\nError writing report file: {e}", file=sys.stderr)


def calculate_acpl_from_analyzed_pgn(pgn_path, player_name):
    """
    Calculates ACPL for a player by reading [%cpl] comments from an analyzed PGN.
    """
    total_player_cpl = 0
    total_player_moves = 0
    cpl_pattern = re.compile(r"\[%cpl (\d+)\]")
    game_count = 0

    print("Step 1: Parsing CPL data from analyzed PGN...")
    try:
        with open(pgn_path, encoding='utf-8') as pgn:
            while True:
                game_count += 1
                print(f"\rProcessing game {game_count}...", end="")
                sys.stdout.flush()

                game = chess.pgn.read_game(pgn)
                if game is None:
                    break
                
                white_player = game.headers.get("White", "").strip()
                black_player = game.headers.get("Black", "").strip()
                
                is_white = white_player.lower() == player_name.lower()
                is_black = black_player.lower() == player_name.lower()

                if not is_white and not is_black:
                    continue

                node = game
                ply_count = 0
                while node.variations:
                    node = node.variation(0)
                    match = cpl_pattern.search(node.comment)
                    if match:
                        cpl = int(match.group(1))
                        
                        if ply_count % 2 == 0 and is_white:
                            total_player_cpl += cpl
                            total_player_moves += 1
                        elif ply_count % 2 != 0 and is_black:
                            total_player_cpl += cpl
                            total_player_moves += 1
                    
                    ply_count += 1
    except Exception as e:
        print(f"\nError reading PGN during parsing phase: {e}", file=sys.stderr)
        return 0, 0
    
    print("\nParsing complete.")
    return total_player_cpl, total_player_moves


def main():
    """Main function to parse arguments and run the analysis."""
    parser = argparse.ArgumentParser(
        description="Analyzes a PGN file with Stockfish or generates an ACPL report from an already analyzed file.",
        formatter_class=argparse.RawTextHelpFormatter,
        epilog=(
            "Full Analysis Example:\n"
            "python cpl2.py --stockfish ./stockfish --pgn games.pgn --duration 9 --player 'YourName'\n\n"
            "Report-Only Example:\n"
            "python cpl2.py --pgn games_analyzed.pgn --player 'YourName' --report-only"
        )
    )
    
    default_stockfish_path = get_stockfish_path()

    parser.add_argument("--pgn", dest="pgn_path", required=True, help="Path to the input PGN file.")
    parser.add_argument("--player", dest="player_name", required=True, help="The username of the player to calculate ACPL for.")
    parser.add_argument("--report", dest="report_path", default="acpl_report.txt", help="Path for the ACPL report file. Defaults to 'acpl_report.txt'.")
    
    # Analysis-specific arguments
    parser.add_argument("--stockfish", dest="stockfish_path", default=default_stockfish_path, help="Path to the Stockfish executable (required for analysis).")
    parser.add_argument("--duration", dest="total_duration_hours", type=float, default=9.0, help="Total desired analysis duration in hours.")
    parser.add_argument("--output", dest="output_path", help="Path for the output PGN file. Defaults to '[input_name]_analyzed.pgn'.")
    parser.add_argument('--report-only', action='store_true', help="Skip analysis and generate ACPL report from an already analyzed PGN.")

    args = parser.parse_args()
    
    if not os.path.exists(args.pgn_path):
        print(f"Error: PGN file not found at '{args.pgn_path}'", file=sys.stderr)
        sys.exit(1)

    if args.report_only:
        # --- REPORT-ONLY MODE ---
        print("\n--- Report-Only Mode ---")
        total_player_cpl, total_player_moves = calculate_acpl_from_analyzed_pgn(args.pgn_path, args.player_name)
        
        print("\nStep 2: Generating ACPL Report...")
        if total_player_moves > 0:
            player_acpl = total_player_cpl / total_player_moves
            print(f"Player '{args.player_name}' found with {total_player_moves} moves.")
            print(f"Average Centipawn Loss (ACPL): {player_acpl:.2f}")
            generate_acpl_report(args.player_name, player_acpl, total_player_moves, args.report_path)
        else:
            print(f"Warning: No moves found for player '{args.player_name}' with CPL data.", file=sys.stderr)
            print("Please check the player name and ensure the PGN file contains '[%cpl ...]' comments.", file=sys.stderr)

    else:
        # --- FULL ANALYSIS MODE ---
        print("\n--- Full Analysis Mode ---")
        if not args.stockfish_path or not os.path.exists(args.stockfish_path):
            print(f"Error: Stockfish executable not found at '{args.stockfish_path}'", file=sys.stderr)
            print("Stockfish path is required for Full Analysis Mode. Use --stockfish or ensure it's in your PATH.", file=sys.stderr)
            sys.exit(1)
        
        start_time = time.time()
        output_path = args.output_path
        if not output_path:
            base, _ = os.path.splitext(args.pgn_path)
            output_path = f"{base}_analyzed.pgn"

        print("Step 1: Counting total moves in PGN file...")
        total_moves = 0
        try:
            with open(args.pgn_path, encoding='utf-8') as pgn:
                while True:
                    game = chess.pgn.read_game(pgn)
                    if game is None: break
                    total_moves += len(list(game.mainline_moves()))
        except Exception as e:
            print(f"\nError reading PGN during counting phase: {e}", file=sys.stderr)
            sys.exit(1)

        if total_moves == 0:
            print("No moves found in the PGN file. Nothing to analyze.", file=sys.stderr)
            sys.exit(0)
        
        print(f"Found {total_moves} moves to analyze.")

        total_duration_seconds = args.total_duration_hours * 3600
        time_per_move = total_duration_seconds / total_moves
        
        print("\n--- Analysis Plan ---")
        print(f"Target Player: {args.player_name}")
        print(f"Target duration: {args.total_duration_hours} hours ({total_duration_seconds:.0f} seconds)")
        print(f"Calculated time per move: {time_per_move:.4f} seconds")
        print("---------------------\n")
        print("Step 2: Starting analysis...")
        
        total_player_cpl = 0
        total_player_moves = 0
        engine = None
        try:
            engine = chess.engine.SimpleEngine.popen_uci(args.stockfish_path)
            analysis_limit = chess.engine.Limit(time=time_per_move)
            
            with open(args.pgn_path, encoding='utf-8') as pgn, open(output_path, "w", encoding="utf-8") as out_pgn:
                game_count = 0
                while True:
                    game_count += 1
                    game = chess.pgn.read_game(pgn)
                    if game is None: break
                    
                    print(f"\rAnalyzing game {game_count}...", end="")
                    sys.stdout.flush()

                    analyzed_game, cpl_data = analyze_game_and_annotate(game, engine, analysis_limit)
                    
                    if cpl_data:
                        white_player = game.headers.get("White", "").strip()
                        black_player = game.headers.get("Black", "").strip()

                        for i, cpl in enumerate(cpl_data):
                            if i % 2 == 0 and white_player.lower() == args.player_name.lower():
                                total_player_cpl += cpl
                                total_player_moves += 1
                            elif i % 2 != 0 and black_player.lower() == args.player_name.lower():
                                total_player_cpl += cpl
                                total_player_moves += 1
                    
                    if analyzed_game:
                        exporter = chess.pgn.FileExporter(out_pgn)
                        analyzed_game.accept(exporter)
            
            actual_duration = time.time() - start_time
            print(f"\n\nAnalysis complete.")
            print(f"Saved analyzed PGN to '{output_path}'.")
            print(f"Total time elapsed: {actual_duration / 3600:.2f} hours.")

            print("\nStep 3: Generating ACPL Report...")
            if total_player_moves > 0:
                player_acpl = total_player_cpl / total_player_moves
                print(f"Player '{args.player_name}' found with {total_player_moves} moves.")
                print(f"Average Centipawn Loss (ACPL): {player_acpl:.2f}")
                generate_acpl_report(args.player_name, player_acpl, total_player_moves, args.report_path)
            else:
                print(f"Warning: No moves found for player '{args.player_name}'. Could not generate ACPL report.", file=sys.stderr)

        except Exception as e:
            print(f"\nAn unexpected error occurred: {e}", file=sys.stderr)
        finally:
            if engine:
                engine.quit()

if __name__ == "__main__":
    main()
</code></pre>

<p>Notice that the directory paths do not include spaces unless they also include quotes. This was a point I had to learn the hard way. I also utilized Google’s Gemini Deepmind to help me with the programming syntax (I am still a beginning Python programmer, myself) and to help get the Leela Chess Engine to host the various versions of the Maia engine weights.</p>
        
        <!-- Final paragraph with mailto link -->
        <p>I hope you enjoyed this analysis. Please feel free to send any comments, suggestions, thoughts, criticisms or insights to <a href="mailto:andyhayles@gmail.com">andyhayles@gmail.com</a>.</p>

      </div> <!-- /prose container -->
    </main>

    <!-- === Footer (Consistent with index.html) === -->
    <footer class="bg-gray-100 dark:bg-gray-900 text-complementary dark:text-onComplementary py-12">
      <div class="container mx-auto px-4 text-center">
        <div class="flex justify-center space-x-6 mb-4">
          <a href="mailto:andyhayles@gmail.com" class="hover:text-primary dark:hover:text-secondary">Contact</a>
          <a href="https://github.com/andrewhayles/" target="_blank" rel="noopener noreferrer" class="hover:text-primary dark:hover:text-secondary">GitHub</a>
        </div>
        <!-- Copyright text from config.json -->
        <p class="text-sm">Powered by <a href="https://www.netlify.com/" target="_blank" rel="noopener noreferrer" class="hover:text-primary dark:hover:text-secondary">Netlify</a></p>
        <p class="text-sm">&copy; 2024 Andrew Hayles. All rights reserved.</p>
      </div>
    </footer>

  </div><!-- /flex-col min-h-screen -->
</body>
</html>
